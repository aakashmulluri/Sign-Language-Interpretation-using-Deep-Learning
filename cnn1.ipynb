{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14010 images belonging to 10 classes.\n",
      "Found 800 images belonging to 10 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/10\n",
      "1401/1401 [==============================] - 58s 40ms/step - loss: 0.1000 - accuracy: 0.9862 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "1401/1401 [==============================] - 55s 39ms/step - loss: 4.5687e-04 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "1401/1401 [==============================] - 55s 39ms/step - loss: 2.4527e-04 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "1401/1401 [==============================] - 55s 40ms/step - loss: 1.6704e-04 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "1401/1401 [==============================] - 55s 39ms/step - loss: 1.2534e-04 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1401/1401 [==============================] - 55s 39ms/step - loss: 9.9515e-05 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "1401/1401 [==============================] - 55s 39ms/step - loss: 8.2659e-05 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "1401/1401 [==============================] - 55s 40ms/step - loss: 7.2570e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "1401/1401 [==============================] - 55s 40ms/step - loss: 6.7364e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1401/1401 [==============================] - 55s 39ms/step - loss: 6.2792e-05 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "loss of 0.00043499324237927794; accuracy of 100.0%\n",
      "{'loss': [0.0999782457947731, 0.00045686610974371433, 0.00024526557535864413, 0.00016704315203242004, 0.0001253432856174186, 9.951453830581158e-05, 8.265938959084451e-05, 7.2569637268316e-05, 6.736415525665507e-05, 6.279164517764002e-05], 'accuracy': [0.9862241148948669, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.0045054019428789616, 0.002842401620000601, 0.0022966049145907164, 0.002036889549344778, 0.0018485977780073881, 0.001644614734686911, 0.0015857162652537227, 0.0015262659871950746, 0.0014887660508975387, 0.0014666884671896696], 'val_accuracy': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0005, 0.0005, 0.0005]}\n",
      "loss of 3.1255629437509924e-05; accuracy of 100.0%\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 31, 31, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 64)                294976    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 414,346\n",
      "Trainable params: 414,346\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "predictions on a small set of test data--\n",
      "\n",
      "4   0   9   7   6   2   0   7   9   2   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "4   0   9   7   6   2   0   7   9   2   (10, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "\n",
    "train_path = 'gestures/train'\n",
    "test_path = 'gestures/test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)\n",
    "\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(10,activation =\"softmax\"))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=10, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save('best_model_dataflair.h5')\n",
    "model.save('best_model.h5')\n",
    "\n",
    "print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(r\"best_model.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "\n",
    "word_dict = {0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9'}\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "#history2.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
