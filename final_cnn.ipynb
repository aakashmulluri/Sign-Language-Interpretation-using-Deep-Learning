{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24799 images belonging to 31 classes.\n",
      "Found 2480 images belonging to 31 classes.\n",
      "{'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'A': 11, 'B': 12, 'C': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17, 'I': 18, 'L': 19, 'Y': 20, 'Yes': 21, 'are': 22, 'hello': 23, 'how': 24, 'is': 25, 'my': 26, 'name': 27, 'no': 28, 'thanks': 29, 'you': 30}\n",
      "{'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'A': 11, 'B': 12, 'C': 13, 'E': 14, 'F': 15, 'G': 16, 'H': 17, 'I': 18, 'L': 19, 'Y': 20, 'Yes': 21, 'are': 22, 'hello': 23, 'how': 24, 'is': 25, 'my': 26, 'name': 27, 'no': 28, 'thanks': 29, 'you': 30}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 64, 64, 3)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/15\n",
      "2480/2480 [==============================] - 95s 38ms/step - loss: 0.2190 - accuracy: 0.9475 - val_loss: 0.0446 - val_accuracy: 0.9847\n",
      "Epoch 2/15\n",
      "2480/2480 [==============================] - 93s 38ms/step - loss: 4.4117e-04 - accuracy: 1.0000 - val_loss: 0.0413 - val_accuracy: 0.9855\n",
      "Epoch 3/15\n",
      "2480/2480 [==============================] - 93s 37ms/step - loss: 2.2687e-04 - accuracy: 1.0000 - val_loss: 0.0394 - val_accuracy: 0.9855\n",
      "Epoch 4/15\n",
      "2480/2480 [==============================] - 88s 36ms/step - loss: 1.5246e-04 - accuracy: 1.0000 - val_loss: 0.0435 - val_accuracy: 0.9855\n",
      "Epoch 5/15\n",
      "2480/2480 [==============================] - 84s 34ms/step - loss: 1.2065e-04 - accuracy: 1.0000 - val_loss: 0.0420 - val_accuracy: 0.9855\n",
      "loss of 0.0005179865984246135; accuracy of 100.0%\n",
      "{'loss': [0.2189616560935974, 0.00044117268407717347, 0.00022687474847771227, 0.0001524551771581173, 0.00012064875772921368], 'accuracy': [0.9474979043006897, 1.0, 1.0, 1.0, 1.0], 'val_loss': [0.04461905360221863, 0.041295092552900314, 0.03935231268405914, 0.04347817972302437, 0.04204924404621124], 'val_accuracy': [0.9846774339675903, 0.9854838848114014, 0.9854838848114014, 0.9854838848114014, 0.9854838848114014], 'lr': [0.001, 0.001, 0.001, 0.001, 0.0005]}\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000002026AACA790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_test_function.<locals>.test_function at 0x000002026AACA790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss of 0.0005955820670351386; accuracy of 100.0%\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_36 (Conv2D)           (None, 62, 62, 16)        448       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 31, 31, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_38 (MaxPooling (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 7, 7, 128)         73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_39 (MaxPooling (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 64)                73792     \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 31)                3999      \n",
      "=================================================================\n",
      "Total params: 200,063\n",
      "Trainable params: 200,063\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002026A929CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002026A929CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions on a small set of test data--\n",
      "\n",
      "I   8   2   2   no   H   6   A   7   8   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x1440 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels\n",
      "I   8   2   2   no   H   6   A   7   8   (10, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.2189616560935974,\n",
       "  0.00044117268407717347,\n",
       "  0.00022687474847771227,\n",
       "  0.0001524551771581173,\n",
       "  0.00012064875772921368],\n",
       " 'accuracy': [0.9474979043006897, 1.0, 1.0, 1.0, 1.0],\n",
       " 'val_loss': [0.04461905360221863,\n",
       "  0.041295092552900314,\n",
       "  0.03935231268405914,\n",
       "  0.04347817972302437,\n",
       "  0.04204924404621124],\n",
       " 'val_accuracy': [0.9846774339675903,\n",
       "  0.9854838848114014,\n",
       "  0.9854838848114014,\n",
       "  0.9854838848114014,\n",
       "  0.9854838848114014],\n",
       " 'lr': [0.001, 0.001, 0.001, 0.001, 0.0005]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import itertools\n",
    "import random\n",
    "import warnings\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "word_dict = {0:'0', 1:'1', 2:'10', 3:'2', 4:'3', 5:'4', 6:'5', 7:'6', 8:'7', 9:'8', 10:'9', 11:'A', 12:'B', 13:'C', 14:'E',\n",
    "             15:'F', 16:'G', 17:'H', 18:'I', 19:'L', 20:'Y', 21:'Yes', 22:'are', 23:'hello', 24:'how',\n",
    "             25:'is', 26:'my', 27:'name', 28:'no', 29:'thanks', 30:'you'}\n",
    "\n",
    "\n",
    "train_path = 'gestures/train'\n",
    "test_path = 'gestures/test'\n",
    "\n",
    "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=train_path, target_size=(64,64), class_mode='categorical', batch_size=10,shuffle=True)\n",
    "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input).flow_from_directory(directory=test_path, target_size=(64,64), class_mode='categorical', batch_size=10, shuffle=True)\n",
    "\n",
    "print(train_batches.class_indices)\n",
    "print(test_batches.class_indices)\n",
    "imgs, labels = next(train_batches)\n",
    "\n",
    "\n",
    "#Plotting the images...\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotImages(imgs)\n",
    "print(imgs.shape)\n",
    "print(labels)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'same'))\n",
    "model.add(MaxPool2D(pool_size=(2, 2), strides=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64,activation =\"relu\"))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(128,activation =\"relu\"))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(31,activation =\"softmax\"))\n",
    "\n",
    "\n",
    "# In[23]:\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0005)\n",
    "early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')\n",
    "\n",
    "\n",
    "history2 = model.fit(train_batches, epochs=15, callbacks=[reduce_lr, early_stop],  validation_data = test_batches)#, checkpoint])\n",
    "imgs, labels = next(train_batches) # For getting next batch of imgs...\n",
    "\n",
    "imgs, labels = next(test_batches) # For getting next batch of imgs...\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "\n",
    "#model.save('best_model_dataflair.h5')\n",
    "model.save('best_model.h5')\n",
    "\n",
    "print(history2.history)\n",
    "\n",
    "imgs, labels = next(test_batches)\n",
    "\n",
    "model = keras.models.load_model(r\"best_model.h5\")\n",
    "\n",
    "scores = model.evaluate(imgs, labels, verbose=0)\n",
    "print(f'{model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "scores #[loss, accuracy] on test data...\n",
    "model.metrics_names\n",
    "\n",
    "predictions = model.predict(imgs, verbose=0)\n",
    "print(\"predictions on a small set of test data--\")\n",
    "print(\"\")\n",
    "for ind, i in enumerate(predictions):\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "plotImages(imgs)\n",
    "print('Actual labels')\n",
    "for i in labels:\n",
    "    print(word_dict[np.argmax(i)], end='   ')\n",
    "\n",
    "print(imgs.shape)\n",
    "\n",
    "history2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
