{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the messageI am fine.\n",
      "program terminated\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import os\n",
    "from threading import Thread\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "from moviepy.editor import *\n",
    "import keyboard\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "\n",
    "\n",
    "background= None\n",
    "def say_text(text):\n",
    "    engine = pyttsx3.init()\n",
    "    while engine._inLoop:\n",
    "        pass\n",
    "    engine.setProperty(\"rate\", 125)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "    global background\n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    global contour\n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)    \n",
    "    #Fetching contours in the frame (These contours can be of hand or any other object in foreground) ...\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # If length of contours list = 0, means we didn't get any contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)  \n",
    "        contour = max(contours, key = cv2.contourArea)\n",
    "        # Returning the hand segment(max contour) and the thresholded image of hand...\n",
    "        return (thresholded, hand_segment_max_cont)\n",
    "\n",
    "def sign_to_speech():\n",
    "    model = keras.models.load_model(\"best_model.h5\")\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    data=\"\"\n",
    "    text=\"\"\n",
    "    count_same_frame = 0\n",
    "    count_same_frame1 = 0\n",
    "    count_same_frame2 = 0\n",
    "    background = None\n",
    "    accumulated_weight = 0.5\n",
    "    ROI_top = 100\n",
    "    ROI_bottom = 300\n",
    "    ROI_right = 150\n",
    "    ROI_left = 350\n",
    "    word_dict = {0:'0', 1:'1', 2:'10', 3:'2', 4:'3', 5:'4', 6:'5', 7:'6', 8:'7', 9:'8', 10:'9', 11:'A', 12:'B', 13:'C', 14:'E',\n",
    "             15:'F', 16:'G', 17:'H', 18:'I', 19:'L', 20:'Y', 21:'Yes', 22:'are', 23:'hello', 24:'how',\n",
    "             25:'is', 26:'my', 27:'name', 28:'no', 29:'thanks for listening', 30:'you'}\n",
    "\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    num_frames =0\n",
    "    with mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n",
    "        while True:\n",
    "            ret, frame = cam.read()\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            image1 = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "        #image1 = cv2.flip(image1, 1)\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "            image1.flags.writeable = False\n",
    "            results = hands.process(image1)\n",
    "            image1.flags.writeable = True\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image1, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            cv2.imshow('MediaPipe Hands', image1)\n",
    "            frame_copy = frame.copy()\n",
    "            roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "            gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "            if num_frames < 151:\n",
    "                cal_accum_avg(gray_frame, accumulated_weight)\n",
    "                cv2.putText(frame_copy, str(num_frames)+\"For\", (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                cv2.putText(frame_copy, \"Saving Backgroung. Please wait for 150 frames\", (80, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
    "                cv2.putText(frame_copy, \"Keep background for better prediction\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
    "         \n",
    "            else: \n",
    "                hand = segment_hand(gray_frame)\n",
    "                if hand is not None:\n",
    "                    thresholded, hand_segment = hand\n",
    "                    cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "                    cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "                    thresholded = cv2.resize(thresholded, (64, 64))\n",
    "                    thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "                    thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "                    pred = model.predict(thresholded)\n",
    "                    cv2.putText(frame_copy, word_dict[np.argmax(pred)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                    old_text=text\n",
    "                    cv2.putText(frame_copy, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.5, (255,0,0), 1)\n",
    "                    if cv2.contourArea(contour) > 4000:\n",
    "                        text = word_dict[np.argmax(pred)]\n",
    "                        if old_text == text:\n",
    "                            count_same_frame += 1\n",
    "                        if old_text!=text:\n",
    "                            count_same_frame=0\n",
    "                        if count_same_frame == 30:\n",
    "                            data = data + text\n",
    "                            if data.startswith('I/Me '):\n",
    "                                data = data.replace('I/Me ', 'I ')\n",
    "                            elif data.endswith('I/Me '):\n",
    "                                data = data.replace('I/Me ', 'me ')\n",
    "                            data+=\" \"\n",
    "                            count_same_frame =0\n",
    "                    elif count_same_frame < 20:\n",
    "                        data+=\".\"\n",
    "                        if data != '':\n",
    "                            Thread(target=say_text, args=(data, )).start()\n",
    "                        text = \"\"\n",
    "                        data = \"\"\n",
    "                        count_same_frame=0\n",
    "                else:\n",
    "                    if data != '':\n",
    "                        Thread(target=say_text, args=(data, )).start()\n",
    "                    text = \"\"\n",
    "                    data = \"\"\n",
    "            blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "            cv2.putText(blackboard, \" \", (180, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 0,0))\n",
    "            cv2.putText(blackboard, \"Predicted text- \" + text, (30, 100), cv2.FONT_HERSHEY_TRIPLEX, 0.75, (255, 255, 0))\n",
    "            cv2.putText(blackboard, data, (10, 240), cv2.FONT_HERSHEY_TRIPLEX, 0.65, (255, 255, 255))\n",
    "            cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "            num_frames += 1\n",
    "            cv2.putText(frame_copy, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.5, (0,255,255), 1)\n",
    "            res = np.hstack((frame_copy, blackboard))\n",
    "            cv2.imshow(\"Recognizing gesture\", res)\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k == 27:\n",
    "                break\n",
    "\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "def text_to_sign():\n",
    "    t=input(\"Enter your message: \")\n",
    "    img=re.split('\\s+', t)\n",
    "    for k in range(len(img)):\n",
    "        img[k]= img[k]+\".jpg\"\n",
    "    print(img)\n",
    "    \n",
    "    while len(img)==1:\n",
    "        res = cv2.imread('D:/data/code/'+str(t)+'.jpg')\n",
    "        cv2.imshow(\"image\", res)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    if len(img)>1:\n",
    "        time.sleep(5)\n",
    "        clips = [ImageClip(m).set_duration(3)\n",
    "                  for m in img]\n",
    "        concat_clip = concatenate_videoclips(clips)\n",
    "        concat_clip.preview()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "        \n",
    "def speech_to_sign():\n",
    "    r = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    with mic as audio_file:\n",
    "        print(\"Speak Please\")\n",
    "        r.adjust_for_ambient_noise(audio_file)\n",
    "        audio = r.listen(audio_file)\n",
    "        t= r.recognize_google(audio)\n",
    "        print(\"Converting Speech to Text...\")\n",
    "        print(\"You said: \" + t)\n",
    "    img=re.split('\\s+', t)\n",
    "    for k in range(len(img)):\n",
    "        img[k]= img[k]+\".jpg\"\n",
    "    print(img)\n",
    "    \n",
    "    while len(img)==1:\n",
    "        res = cv2.imread('D:/data/code/'+str(t)+'.jpg')\n",
    "        cv2.imshow(\"image\", res)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    if len(img)>1:\n",
    "        time.sleep(5)\n",
    "        clips = [ImageClip(m).set_duration(3)\n",
    "                  for m in img]\n",
    "        concat_clip = concatenate_videoclips(clips)\n",
    "        concat_clip.preview()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "def text_to_speech():\n",
    "    msg=input(\"enter the message\")\n",
    "    say_text(msg)\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "            text_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "\n",
    "while True:\n",
    "    if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "        text_to_sign()\n",
    "        break\n",
    "    if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "        speech_to_sign()\n",
    "        break\n",
    "    if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "        sign_to_speech()\n",
    "        break\n",
    "    if keyboard.is_pressed('t'):  # if key 't' is pressed \n",
    "        text_to_speech()\n",
    "        break\n",
    "    if keyboard.is_pressed('r'):\n",
    "        print(\"program terminated\")# if key 'r' is pressed \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
