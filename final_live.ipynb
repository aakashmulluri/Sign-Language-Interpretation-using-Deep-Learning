{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your message: hello how are you\n",
      "['hello.jpg', 'how.jpg', 'are.jpg', 'you.jpg']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: 'D:\\data\\code\\hello.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-39ed864e8aad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'q'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if key 'q' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m         \u001b[0mtext_to_sign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkeyboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_pressed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# if key 'w' is pressed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-39ed864e8aad>\u001b[0m in \u001b[0;36mtext_to_sign\u001b[1;34m()\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         clips = [ImageClip(m).set_duration(3)\n\u001b[0m\u001b[0;32m    189\u001b[0m                   for m in img]\n\u001b[0;32m    190\u001b[0m         \u001b[0mconcat_clip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate_videoclips\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclips\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-39ed864e8aad>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         clips = [ImageClip(m).set_duration(3)\n\u001b[0m\u001b[0;32m    189\u001b[0m                   for m in img]\n\u001b[0;32m    190\u001b[0m         \u001b[0mconcat_clip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcatenate_videoclips\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclips\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\moviepy\\video\\VideoClip.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, img, ismask, transparent, fromalpha, duration)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# img is (now) a RGB(a) numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mimread\u001b[1;34m(uri, format, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[1;31m# Get reader and read first\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m     \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[1;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# Create request object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m     \u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;31m# Get format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;31m# Parse what was given\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muri\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# Set extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imageio\\core\\request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[1;34m(self, uri)\u001b[0m\n\u001b[0;32m    258\u001b[0m                 \u001b[1;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file: '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                 \u001b[1;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: No such file: 'D:\\data\\code\\hello.jpg'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import os\n",
    "from threading import Thread\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import pyttsx3\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "from gtts import gTTS \n",
    "from playsound import playsound \n",
    "import cv2\n",
    "import re\n",
    "import time\n",
    "from moviepy.editor import *\n",
    "import keyboard\n",
    "import speech_recognition as sr\n",
    "import time\n",
    "\n",
    "\n",
    "background= None\n",
    "def say_text(text):\n",
    "    engine = pyttsx3.init()\n",
    "    while engine._inLoop:\n",
    "        pass\n",
    "    engine.setProperty(\"rate\", 135)\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "    global background\n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)\n",
    "\n",
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "    global contour\n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)    \n",
    "    #Fetching contours in the frame (These contours can be of hand or any other object in foreground) ...\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # If length of contours list = 0, means we didn't get any contours...\n",
    "    if len(contours) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        # The largest external contour should be the hand \n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)  \n",
    "        contour = max(contours, key = cv2.contourArea)\n",
    "        # Returning the hand segment(max contour) and the thresholded image of hand...\n",
    "        return (thresholded, hand_segment_max_cont)\n",
    "\n",
    "def sign_to_speech():\n",
    "    model = keras.models.load_model(\"best_model.h5\")\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_hands = mp.solutions.hands\n",
    "    data=\"\"\n",
    "    text=\"\"\n",
    "    count_same_frame = 0\n",
    "    count_same_frame1 = 0\n",
    "    count_same_frame2 = 0\n",
    "    background = None\n",
    "    accumulated_weight = 0.5\n",
    "    ROI_top = 100\n",
    "    ROI_bottom = 300\n",
    "    ROI_right = 150\n",
    "    ROI_left = 350\n",
    "    word_dict = {0:'0', 1:'1', 2:'10', 3:'2', 4:'3', 5:'4', 6:'5', 7:'6', 8:'7', 9:'8', 10:'9', 11:'A', 12:'B', 13:'C', 14:'E',\n",
    "             15:'F', 16:'G', 17:'H', 18:'I', 19:'L', 20:'Y', 21:'Yes', 22:'are', 23:'hello', 24:'how',\n",
    "             25:'is', 26:'my', 27:'name', 28:'no', 29:'thanks for listening', 30:'you'}\n",
    "\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    num_frames =0\n",
    "    with mp_hands.Hands(min_detection_confidence=0.5,min_tracking_confidence=0.5) as hands:\n",
    "        while True:\n",
    "            ret, frame = cam.read()\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            image1 = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "        #image1 = cv2.flip(image1, 1)\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_BGR2RGB)\n",
    "            image1.flags.writeable = False\n",
    "            results = hands.process(image1)\n",
    "            image1.flags.writeable = True\n",
    "            image1 = cv2.cvtColor(image1, cv2.COLOR_RGB2BGR)\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    mp_drawing.draw_landmarks(image1, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            cv2.imshow('MediaPipe Hands', image1)\n",
    "            frame_copy = frame.copy()\n",
    "            roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "            gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "            if num_frames < 151:\n",
    "                cal_accum_avg(gray_frame, accumulated_weight)\n",
    "                cv2.putText(frame_copy, str(num_frames)+\"For\", (70, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                cv2.putText(frame_copy, \"Saving Backgroung. Please wait for 150 frames\", (80, 350), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
    "                cv2.putText(frame_copy, \"Keep background for better prediction\", (80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 2)\n",
    "         \n",
    "            else: \n",
    "                hand = segment_hand(gray_frame)\n",
    "                if hand is not None:\n",
    "                    thresholded, hand_segment = hand\n",
    "                    cv2.drawContours(frame_copy, [hand_segment + (ROI_right, ROI_top)], -1, (255, 0, 0),1)\n",
    "                    cv2.imshow(\"Thesholded Hand Image\", thresholded)\n",
    "                    thresholded = cv2.resize(thresholded, (64, 64))\n",
    "                    thresholded = cv2.cvtColor(thresholded, cv2.COLOR_GRAY2RGB)\n",
    "                    thresholded = np.reshape(thresholded, (1,thresholded.shape[0],thresholded.shape[1],3))\n",
    "                    pred = model.predict(thresholded)\n",
    "                    cv2.putText(frame_copy, word_dict[np.argmax(pred)], (170, 45), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "                    old_text=text\n",
    "                    cv2.putText(frame_copy, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.5, (255,0,0), 1)\n",
    "                    if cv2.contourArea(contour) > 4000:\n",
    "                        text = word_dict[np.argmax(pred)]\n",
    "                        if old_text == text:\n",
    "                            count_same_frame += 1\n",
    "                        if old_text!=text:\n",
    "                            count_same_frame=0\n",
    "                        if count_same_frame == 30:\n",
    "                            data = data + text\n",
    "                            if data.startswith('I/Me '):\n",
    "                                data = data.replace('I/Me ', 'I ')\n",
    "                            elif data.endswith('I/Me '):\n",
    "                                data = data.replace('I/Me ', 'me ')\n",
    "                            data+=\" \"\n",
    "                            count_same_frame =0\n",
    "                    elif count_same_frame < 20:\n",
    "                        data+=\".\"\n",
    "                        if data != '':\n",
    "                            Thread(target=say_text, args=(data, )).start()\n",
    "                        text = \"\"\n",
    "                        data = \"\"\n",
    "                        count_same_frame=0\n",
    "                else:\n",
    "                    if data != '':\n",
    "                        Thread(target=say_text, args=(data, )).start()\n",
    "                    text = \"\"\n",
    "                    data = \"\"\n",
    "            blackboard = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "            cv2.putText(blackboard, \" \", (180, 50), cv2.FONT_HERSHEY_TRIPLEX, 1.5, (255, 0,0))\n",
    "            cv2.putText(blackboard, \"Predicted text- \" + text, (30, 100), cv2.FONT_HERSHEY_TRIPLEX, 0.75, (255, 255, 0))\n",
    "            cv2.putText(blackboard, data, (10, 240), cv2.FONT_HERSHEY_TRIPLEX, 0.65, (255, 255, 255))\n",
    "            cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right, ROI_bottom), (255,128,0), 3)\n",
    "            num_frames += 1\n",
    "            cv2.putText(frame_copy, str(count_same_frame), (30, 20), cv2.FONT_ITALIC, 0.5, (0,255,255), 1)\n",
    "            res = np.hstack((frame_copy, blackboard))\n",
    "            cv2.imshow(\"Recognizing gesture\", res)\n",
    "            k = cv2.waitKey(1) & 0xFF\n",
    "            if k == 27:\n",
    "                break\n",
    "\n",
    "        cam.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "def text_to_sign():\n",
    "    t=input(\"Enter your message: \")\n",
    "    img=re.split('\\s+', t)\n",
    "    for k in range(len(img)):\n",
    "        img[k]= img[k]+\".jpg\"\n",
    "    print(img)\n",
    "    \n",
    "    while len(img)==1:\n",
    "        res = cv2.imread('D:/data/code/'+str(t)+'.jpg')\n",
    "        cv2.imshow(\"image\", res)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    if len(img)>1:\n",
    "        time.sleep(5)\n",
    "        clips = [ImageClip(m).set_duration(3)\n",
    "                  for m in img]\n",
    "        concat_clip = concatenate_videoclips(clips)\n",
    "        concat_clip.preview()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "        \n",
    "def speech_to_sign():\n",
    "    r = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "    with mic as audio_file:\n",
    "        print(\"Speak Please\")\n",
    "        r.adjust_for_ambient_noise(audio_file)\n",
    "        audio = r.listen(audio_file)\n",
    "        t= r.recognize_google(audio)\n",
    "        print(\"Converting Speech to Text...\")\n",
    "        print(\"You said: \" + t)\n",
    "    img=re.split('\\s+', t)\n",
    "    for k in range(len(img)):\n",
    "        img[k]= img[k]+\".jpg\"\n",
    "    print(img)\n",
    "    \n",
    "    while len(img)==1:\n",
    "        res = cv2.imread('D:/data/code/'+str(t)+'.jpg')\n",
    "        cv2.imshow(\"image\", res)\n",
    "        k = cv2.waitKey(1) & 0xFF\n",
    "        if k == 27:\n",
    "            break\n",
    "    if len(img)>1:\n",
    "        time.sleep(5)\n",
    "        clips = [ImageClip(m).set_duration(3)\n",
    "                  for m in img]\n",
    "        concat_clip = concatenate_videoclips(clips)\n",
    "        concat_clip.preview()\n",
    "    while True:\n",
    "        if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "            text_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "            speech_to_sign()\n",
    "            break\n",
    "        if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "            sign_to_speech()\n",
    "            break\n",
    "        if keyboard.is_pressed('r'):\n",
    "            print(\"program terminated\")# if key 'r' is pressed \n",
    "            break\n",
    "\n",
    "\n",
    "while True:\n",
    "    if keyboard.is_pressed('q'):  # if key 'q' is pressed \n",
    "        text_to_sign()\n",
    "        break\n",
    "    if keyboard.is_pressed('w'):  # if key 'w' is pressed \n",
    "        speech_to_sign()\n",
    "        break\n",
    "    if keyboard.is_pressed('e'):  # if key 'e' is pressed \n",
    "        sign_to_speech()\n",
    "        break\n",
    "    if keyboard.is_pressed('r'):\n",
    "        print(\"program terminated\")# if key 'r' is pressed \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
